# Flare AI Social

Flare AI Kit template for Social AI Agents.

## ğŸš€ Key Features

- **AI-Powered Social Response**: Automatically monitor and respond to mentions across Twitter/X and Telegram using Gemini AI
- **Custom Model Fine-tuning**: Train personalized models using your own dataset or provided examples
- **TEE Security Integration**: Run in Trusted Execution Environment for hardware-level security
- **Multi-Platform Support**: Single interface to manage multiple social media platforms with rate limiting and retry mechanisms

## ğŸ¯ Getting Started

1. **Build with Docker**:
   ```bash
   # Build the image
   docker build -t flare-ai-social .
   
   # Run the container
   docker run -p 80:80 -it --env-file .env flare-ai-social
   ```

2. **Access UI**: Navigate to `http://localhost:80`

## ğŸ›  Build Manually

### Fine tuning a model over a dataset

1. **Prepare Environment File**: Rename `.env.example` to `.env` and update these model fine-tuning parameters:

| Parameter | Description | Default |
|-----------|-------------|---------|
| `tuned_model_name` | Name of the newly tuned model | pugo-hilion |
| `tuning_source_model` | Name of the foundational model to tune on | models/gemini-1.5-flash-001-tuning |
| `epoch_count` | Number of tuning epochs to run. An epoch is a pass over the whole dataset | 30 |
| `batch_size` | Number of examples to use in each training batch | 4 |
| `learning_rate` | Step size multiplier for the gradient updates | 0.001 |


2. **Prepare a dataset:**
   An example dataset is provided in `src/data/training_data.json`, which consists of tweets from
   [Hugo Philion's X](https://x.com/HugoPhilion) account. You can use any publicly available dataset
   for model fine-tuning.

3. **Tune a new model**
   Set the name of the new tuned model in `src/flare_ai_social/tune_model.py`, then:

   ```bash
   uv run start-tuning
   ```

4. **Observe loss parameters:**
   After tuning in complete, a training loss PNG will be saved in the root folder corresponding to the new model.
   Ideally the loss should minimize to near 0 after several training epochs.

   ![pugo-hilion_mean_loss](https://github.com/user-attachments/assets/f6c4d82b-678a-4ae5-bfb7-39dc59e1103d)

5. **Test the new model**
   Select the new tuned model and compare it against a set of prompting techniques (zero-shot, few-shot and chain-of-thought):

   ```bash
   uv run start-compare
   ```

6. **Start Social Bots**:
   - Set up Twitter/X API credentials
   - Configure Telegram bot token
   - Enable/disable platforms as needed
   ```bash
   uv run start-bots
   ```

### Build using Docker (Recommended)

**Note:** You can only perform this step once you have finishing training a new model.

The Docker setup mimics a TEE environment and includes an Nginx server for routing, while Supervisor manages both the backend and frontend services in a single container.

1. **Build the Docker Image:**

   ```bash
   docker build -t flare-ai-social .
   ```

2. **Run the Docker Container:**

   ```bash
   docker run -p 80:80 -it --env-file .env flare-ai-social
   ```

3. **Access the Frontend:**  
   Open your browser and navigate to [http://localhost:80](http://localhost:80) to interact with the Chat UI.

## ğŸ“ Repo Structure

```
src/flare_ai_social/
â”œâ”€â”€ ai/                            # AI Provider implementations
â”‚   â”œâ”€â”€ base.py                    # Base AI provider abstraction
â”‚   â”œâ”€â”€ gemini.py                  # Google Gemini integration
â”‚   â””â”€â”€ openrouter.py             # OpenRouter integration
â”œâ”€â”€ api/                           # API layer
â”‚   â””â”€â”€ routes/                    # API endpoint definitions
â”œâ”€â”€ attestation/                   # TEE attestation implementation
â”‚   â”œâ”€â”€ vtpm_attestation.py       # vTPM client
â”‚   â””â”€â”€ vtpm_validation.py        # Token validation
â”œâ”€â”€ prompts/                       # Prompt engineering templates
â”‚   â””â”€â”€ templates.py              # Different prompt strategies
â”œâ”€â”€ telegram/                      # Telegram bot implementation
â”‚   â””â”€â”€ service.py                # Telegram service logic
â”œâ”€â”€ twitter/                       # Twitter bot implementation
â”‚   â””â”€â”€ service.py                # Twitter service logic
â”œâ”€â”€ bot_manager.py                # Bot orchestration
â”œâ”€â”€ main.py                       # FastAPI application
â”œâ”€â”€ settings.py                   # Configuration settings
â””â”€â”€ tune_model.py                 # Model fine-tuning utilities
```

## ğŸš€ Deploy on TEE

Deploy on a [Confidential Space](https://cloud.google.com/confidential-computing/confidential-space/docs/confidential-space-overview) using AMD SEV.

### Prerequisites

- **Google Cloud Platform Account:**  
  Access to the [`verifiable-ai-hackathon`](https://console.cloud.google.com/welcome?project=verifiable-ai-hackathon) project is required.

- **Gemini API Key:**  
  Ensure your [Gemini API key](https://aistudio.google.com/app/apikey) is linked to the project.

- **gcloud CLI:**  
  Install and authenticate the [gcloud CLI](https://cloud.google.com/sdk/docs/install).

### Environment Configuration

1. **Set Environment Variables:**  
   Update your `.env` file with:

   ```bash
   TEE_IMAGE_REFERENCE=ghcr.io/flare-foundation/flare-ai-social:main  # Replace with your repo build image
   INSTANCE_NAME=<PROJECT_NAME-TEAM_NAME>
   ```

2. **Load Environment Variables:**

   ```bash
   source .env
   ```

   > **Reminder:** Run the above command in every new shell session.

3. **Verify the Setup:**

   ```bash
   echo $TEE_IMAGE_REFERENCE # Expected output: Your repo build image
   ```

### Deploying to Confidential Space

Run the following command:

```bash
gcloud compute instances create $INSTANCE_NAME \
  --project=verifiable-ai-hackathon \
  --zone=us-central1-a \
  --machine-type=n2d-standard-2 \
  --network-interface=network-tier=PREMIUM,nic-type=GVNIC,stack-type=IPV4_ONLY,subnet=default \
  --metadata=tee-image-reference=$TEE_IMAGE_REFERENCE,\
tee-container-log-redirect=true,\
tee-env-GEMINI_API_KEY=$GEMINI_API_KEY,\
tee-env-TUNED_MODEL_NAME=$TUNED_MODEL_NAME,\
  --maintenance-policy=MIGRATE \
  --provisioning-model=STANDARD \
  --service-account=confidential-sa@verifiable-ai-hackathon.iam.gserviceaccount.com \
  --scopes=https://www.googleapis.com/auth/cloud-platform \
  --min-cpu-platform="AMD Milan" \
  --tags=flare-ai,http-server,https-server \
  --create-disk=auto-delete=yes,\
boot=yes,\
device-name=$INSTANCE_NAME,\
image=projects/confidential-space-images/global/images/confidential-space-debug-250100,\
mode=rw,\
size=11,\
type=pd-standard \
  --shielded-secure-boot \
  --shielded-vtpm \
  --shielded-integrity-monitoring \
  --reservation-affinity=any \
  --confidential-compute-type=SEV
```

#### Post-deployment

After deployment, you should see an output similar to:

```plaintext
NAME          ZONE           MACHINE_TYPE    PREEMPTIBLE  INTERNAL_IP  EXTERNAL_IP    STATUS
social-team1   us-central1-a  n2d-standard-2               10.128.0.18  34.41.127.200  RUNNING
```

It may take a few minutes for Confidential Space to complete startup checks.
You can monitor progress via the [GCP Console](https://console.cloud.google.com/welcome?project=verifiable-ai-hackathon) by clicking **Serial port 1 (console)**.
When you see a message like:

```plaintext
INFO:     Uvicorn running on http://0.0.0.0:8080 (Press CTRL+C to quit)
```

the container is ready. Navigate to the external IP of the instance (visible in the GCP Console) to access the Chat UI.

### ğŸ”§ Troubleshooting

If you encounter issues, follow these steps:

1. **Check Logs:**

   ```bash
   gcloud compute instances get-serial-port-output $INSTANCE_NAME --project=verifiable-ai-hackathon
   ```

2. **Verify API Key(s):**  
   Ensure that all API Keys are set correctly (e.g. `GEMINI_API_KEY`).

3. **Check Firewall Settings:**  
   Confirm that your instance is publicly accessible on port `80`.

## ğŸ’¡ Next Steps

TODO